---
title: "LendingClub Statistical Analysis"
author: "Huiting Sheng, Hua Zou, Lixin Wang"
date: "12/4/2020"
output: html_document
---

## Loading objects and data
```{r cache=T}
packages=c("caret","ggplot2", "tidyverse", "dplyr", "corrplot","e1071", "reshape2","lubridate","usmap", "glmnet", "pROC","doParallel", "ranger","lattice")
# Now load or install & load all
package.check <- lapply(packages,FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

data=read.csv("LoanStats3a.csv", header=T)
data=as_tibble(data)
```

## Data Cleaning and Exploratory Analysis
```{r emptycolumns}
# delete columns that have no values
emptycols=c()
for (i in 1:ncol(data)){
  if (all(is.na(data[,i]))) {
    emptycols=c(emptycols,colnames(data[,i]))
  }
}

dataEmptyColsRm = data %>%select(-emptycols)
```

```{r deleteMinorVariability}
# delete columns with minor variability, keep total_rec_late_fee, pub_rec_bankruptcies
colMinorVar = dataEmptyColsRm %>% preProcess(method='nzv')
rm = colMinorVar $ method $ remove
dataMinorRm = dataEmptyColsRm %>% select(-rm[-c(3,10)])
```

```{r removeUnimportColumns}
# Remove some unimportant or unrelated columns
colList = c(colnames(dataMinorRm)[grep("*_d",names(dataMinorRm))],"id","member_id","url","desc","title","emp_title","zip_code",colnames(dataMinorRm)[grep("*_inv",names(dataMinorRm))],"mths_since_last_record","total_rec_prncp","total_rec_late_fee","total_rec_int", "total_pymnt", "last_pymnt_amnt","recoveries", "sub_grade","funded_amnt" )

loanDf = dataMinorRm %>%
  select(-all_of(colList)) 
```

```{r deleteMinorVariablility}
# continue to delete factor columns with minor variability, keeping term
colWithMinorVar = loanDf %>% select_if(~n_distinct(.) < 4) %>% colnames(.)
loanData = loanDf %>% select(-colWithMinorVar[-1])
```

```{r ConvertVariable}
# Replace NA with 0 in pub_rec_bankruptcies
loanData$pub_rec_bankruptcies[is.na(loanData$pub_rec_bankruptcies)] = 0

# Convert revol_util from factor to numeric
loanData$revol_util = as.numeric(sub("%","",loanData$revol_util))/100

# drop all entry that have null value
loanDataDropNA = loanData %>% drop_na()
anyNA(loanDataDropNA)

# calculate the credit line years of history
convert_year <- function(x, year=1940){
  m <- year(x) %% 100
  year(x) <- ifelse(m > year %% 100, 1900+m, 2000+m)
  x
}

loanDataDropNA$cr_year = loanDataDropNA$earliest_cr_line %>% 
  paste0("01-",.) %>%   # add 01 before the date
  as.Date(format="%d-%B-%y") %>%  # transform the format to year month date
  convert_year () %>% 
  difftime(Sys.Date(), .) %>%  # time differences in years
  time_length("years") %>%
  round(0)

# Drop earliest_cr_line and graph in ggplot
loanDataDropNA=loanDataDropNA %>% select(-earliest_cr_line)
```

## data exploration 
```{r data exploration}
# cr_year
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = cr_year))
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = inq_last_6mths))
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = delinq_2yrs))
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = open_acc))
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = pub_rec))
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = pub_rec_bankruptcies))
ggplot(data=loanDataDropNA) + geom_bar(mapping = aes(x = emp_length))
ggplot(data=loanDataDropNA,aes(y = loan_status)) + geom_bar()
```

## data grouping
```{r data grouping}
# grouping data
loanDataConvert = loanDataDropNA %>%
  mutate(inquiries = case_when(inq_last_6mths == 0 ~ "0", 
                               inq_last_6mths == 1 ~ "1",
                               inq_last_6mths == 2 ~ "2", 
                               inq_last_6mths == 3 ~ "3",
                               inq_last_6mths >= 4 ~ ">=4"),
         delinquency = case_when(delinq_2yrs == 0 ~ "good", 
                                 delinq_2yrs >= 1 ~ "not good"),
         pubRecords = case_when(pub_rec == 0 ~ "good",
                                pub_rec >= 1 ~ "not good"),
        bankruptcies  = case_when(pub_rec_bankruptcies == 0 ~ "good", 
                                 pub_rec_bankruptcies >= 1 ~ "not good"),
        loanStatus = if_else(loan_status %in% 
                              c("Fully Paid"," Does not meet the credit policy. Status:Fully Paid ","Current"), "non_default","default"))%>% select(-inq_last_6mths,-delinq_2yrs,-pub_rec, -pub_rec_bankruptcies, -loan_status)

# Convert character to factor
loanDataConvert$inquiries=as.factor(loanDataConvert$inquiries)
loanDataConvert$delinquency=as.factor(loanDataConvert$delinquency)
loanDataConvert$pubRecords=as.factor(loanDataConvert$pubRecords)
loanDataConvert$bankruptcies=as.factor(loanDataConvert$bankruptcies)
loanDataConvert$loanStatus=as.factor(loanDataConvert$loanStatus)
```

## feature analysis and visualization
```{r grade}
grade=loanDataConvert %>%
  select(grade,loanStatus) %>%
  group_by(grade) %>%
  count(loanStatus) %>%
  mutate(percentage=n/sum(n))

ggplot(grade, aes(fill=factor(loanStatus), x=grade, y=percentage))+geom_bar(stat="identity")
```

```{r term}
term = loanDataConvert %>%
  select(term,loanStatus) %>%
  group_by(term) %>%
  count(loanStatus) %>%
  mutate(percentage=n/sum(n))

ggplot(term, aes(fill=factor(loanStatus), x=term, y=percentage))+geom_bar(stat="identity")
```

```{r employment lengthr}
term = loanDataConvert %>%
  select(emp_length,loanStatus) %>%
  group_by(emp_length) %>%
  count(loanStatus) %>% 
  mutate(percentage=n/sum(n))

ggplot(term, aes(fill=factor(loanStatus), x=emp_length, y=percentage))+geom_bar(stat="identity")
```

```{r inquires}
inquiries = loanDataConvert %>%
  select(inquiries,loanStatus) %>%
  group_by(inquiries) %>%
  count(loanStatus) %>% 
  mutate(percentage=n/sum(n))

ggplot(inquiries, aes(fill=factor(loanStatus), x=inquiries, y=percentage))+geom_bar(stat="identity")
```

```{r purpose}
purpose=loanDataConvert %>%
  select(purpose,loanStatus) %>%
  group_by(purpose) %>%
  count(loanStatus) %>%
  mutate(percentage=n/sum(n))

ggplot(purpose, aes(fill=factor(loanStatus), y=purpose, x=percentage))+geom_bar(stat="identity")
```

```{r map cache = TRUE}
# Group the state and calculate the percentage of the default loan in each state
summary <- loanDataConvert %>%
  group_by(addr_state) %>%
  summarise(percentage = (sum(loanStatus == "default")/(sum(loanStatus == "default")+sum(loanStatus == "non_default"))))

summary = summary[c(2,1,3:50),]

library(usmap)

summary$full = summary$addr_state
summary$full = as.factor(summary$full)
levels(summary$full) = c("","Alaska","Alabama","Arkansas", "Arizona", "California","Colorado","Connecticut","District of Columbia","Delaware","Florida","Georgia","Hawaii","Iowa","Idaho","Illinois","Indiana","Kansas","Kentucky","Louisiana","Massachusetts","Maryland","Maine","Michigan","Minnesota","Missouri","Mississippi","Montana","North Carolina","Nebraska","New Hampshire","New Jersey","New Mexico","Nevada","New York","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Virginia","Vermont","Washington","Wisconsin","West Virginia","Wyoming","")
us_states = usmap::us_map("state")

loanstatusperstate = merge(us_states, summary, by="full")[,c(8,11)]

plot_usmap(data=loanstatusperstate, labels = TRUE, values="percentage") + theme(legend.position = "right") + scale_fill_continuous(name = "Default Loan Percentage") + labs(title = "Default Loan Percentage by States")
```

## data preprocessing
```{r X preprocessing}

# Separate dataset into numeric and factor. 
loanFactor = loanDataConvert %>%
  select_if(is.factor)  %>% 
  select(-c(grade, int_rate))

loanQuan = loanDataConvert %>%
  select_if(is.numeric)

# Separate Y
Y = loanFactor$loanStatus %>% unlist(.)

loanFactor = loanFactor %>% select(-c(loanStatus))

# Convert factor into dummy variables
dummyModel        = dummyVars(~ ., data = loanFactor, fullRank = TRUE) 
loanFactorDummy   = predict(dummyModel, loanFactor)
loanFactorDummy   = as_tibble(loanFactorDummy)

# checking skewness
skewnessVec = loanQuan %>% sapply(., skewness)

names(loanQuan)[abs(skewnessVec) > 2]

# Correct the skewness
loanQuanyeoJ = loanQuan %>%
  preProcess(method  = 'YeoJohnson') %>%
  predict(newdata    = loanQuan)

# Standardize numerical variables and combine with dummy variables
standarded.loanQuanyeoJ = loanQuanyeoJ %>% 
  preProcess(method = c("center", "scale")) %>%
  predict(loanQuanyeoJ)

X = cbind(standarded.loanQuanyeoJ, loanFactorDummy)
```

## split data into training set and test set
```{r split data}
set.seed(1)
trainIndex = caret::createDataPartition(Y, p = .8, 
                                  list = FALSE, 
                                  times = 1) %>% as.vector
Xtrain = X[trainIndex,]
Xtest  = X[-trainIndex,]
Ytrain = Y[trainIndex]
Ytest  = Y[-trainIndex]
```

## Building models

### Elstic net model
```{r elastic net}
cl=makeCluster(6)
registerDoParallel(cl)
set.seed = 1
K = 10
trainControl = trainControl(method = "cv", number = K)
tune.grid = expand.grid(alpha  = seq(0,1, 0.1),lambda = seq(1e-5, 0.01, length = 10))
                              
elasticOut = train(x = Xtrain,  y = relevel(Ytrain, ref = 'non_default'), method = "glmnet", trControl = trainControl, tuneGrid=tune.grid)
elasticOut$bestTune

plot(elasticOut, xlab="alpha", ylab= "K-fold CV", main="Accuracy")
```

```{r elasticnet measurement}

glmnetOut = glmnet(x=as.matrix(Xtrain),   y = relevel(Ytrain, ref = 'non_default') , alpha= elasticOut$bestTune$alpha,lambda =elasticOut$bestTune$lambda,  family='binomial')

probHatTest.response= predict(glmnetOut, as.matrix(Xtest),type='response')

# Set the threshold to 0.2
YhatTestGlmnet=ifelse(probHatTest.response > 0.2, "default","non_default") %>% as.factor

confusionMatrix(data=YhatTestGlmnet, reference = Ytest)

table(YhatTestGlmnet,Ytest)

probHatTest.prob = predict(elasticOut, Xtest,  type="prob")
rocOut.glmnet= roc(response = relevel(Ytest, ref="non_default"), probHatTest.prob$default)

plot(rocOut.glmnet, legacy.axes = TRUE, main = paste("ROC curve using","(N = ",nrow(Xtest),")"))
plot(rocOut.glmnet, print.thres = c(.1,0.2,0.25,0.3,0.5), legacy.axes = TRUE)
rocOut.glmnet$auc

betaHat.glmnet = coef(glmnetOut,s=elasticOut$bestTune$lambda)

confusionMatrix(data=YhatTestGlmnet, reference = Ytest)
```

### Boosting with kappa
```{r boosting with kappa}
cl <- makeCluster(6)
registerDoParallel(cl)
on.exit(stopCluster(cl))

trControl = trainControl(method = "cv")

# Adjusting nrounds and max_depth to find the best tuneGrid
tuneGrid = data.frame('nrounds' = seq(2000, 5000, length.out = 10),
                      'max_depth' = 6,
                      'eta' = 0.01,
                      'gamma' = 0,
                      'colsample_bytree' = 1,
                      'min_child_weight' = 0,
                      'subsample' = 0.5)

boostKappaOut   = train(x = Xtrain, y = relevel(Ytrain, ref = 'default'),
                     method = 'xgbTree', 
                     tuneGrid = tuneGrid,
                     metric = 'Kappa',
                     trControl = trControl)

plot(boostKappaOut)

YhatKappa = relevel(factor(predict(boostKappaOut, Xtest)), ref = 'default')
YtestRelevel = relevel(Ytest, ref="default")

table(YhatKappa, YtestRelevel)
```

### Boosting with Accuracy
```{r boosting with accracy}
cl <- makeCluster(6)
registerDoParallel(cl)
on.exit(stopCluster(cl))

trControl = trainControl(method = "cv")
tuneGrid = data.frame('nrounds' = seq(3000, 5000, length.out = 10),
                      'max_depth' = 2,
                      'eta' = 0.01,
                      'gamma' = 0,
                      'colsample_bytree' = 1,
                      'min_child_weight' = 0,
                      'subsample' = 0.5)

boostOut = train(x = Xtrain, y = relevel(Ytrain, ref = 'default'),
                 method = 'xgbTree',
                 tuneGrid = tuneGrid,
                 metric = 'Accuracy',
                 trControl = trControl)
plot(boostOut)

Yhat = relevel(factor(predict(boostOut,Xtest)), ref = 'default')
table(Yhat, YtestRelevel)
```

### Comparing models
```{r comparing models}
require(pROC)
pHat   = predict(boostOut, Xtest, type = 'prob')
rocOut.boost = roc(YtestRelevel, pHat[,1],levels=c('non_default', 'default'))
plot(rocOut.boost, print.thres = c(.1,0.2,0.25,0.3,0.5), legacy.axes = TRUE)

YhatTest1 = ifelse(pHat[,1] > 0.2, "default","non_default") %>% as.factor
confusionMatrix(YhatTest1, YtestRelevel)
rocOut.boost$auc

pHatKappa   = predict(boostKappaOut, Xtest, type = 'prob')
rocOutKappa = roc(YtestRelevel,pHatKappa[,1],levels=c('non_default','default'))
plot(rocOutKappa, print.thres = c(.1,0.2,0.25,0.3,0.5), legacy.axes = TRUE)

YhatTest2 = ifelse(pHatKappa[,1] > 0.2, "default","non_default") %>% as.factor
confusionMatrix(YhatTest2, YtestRelevel)
rocOutKappa$auc
```

### Feature importance
```{r feature importamce}
library(vip)
elesticnetImportance = vip(elasticOut)
boostKappaImportance = vip(boostKappaOut)
boostImportance      = vip(boostOut)
```
